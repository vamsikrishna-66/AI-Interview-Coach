# -*- coding: utf-8 -*-
"""T5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IgWmy9zQumAO7kYrMP_IE5ruNmmyNRHJ
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

root_path = "/content/drive/MyDrive/jay/final"

# STEP 1: Install dependencies (for Colab/local)
!pip install transformers datasets --quiet

from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset

import os
import re
import pandas as pd

root_path = "/content/drive/MyDrive/final"  # Change this to your folder

def extract_qa_pairs(text):
    pattern = re.compile(
        r'Direct Question:\s*(.*?)\s*Direct Answer:\s*(.*?)\s*Source:\s*(.*?)\s*Type:\s*(\d+)',
        re.DOTALL
    )
    return pattern.findall(text)

def parse_txt_file(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()

    jd_start = content.find('--- JOB DESCRIPTION ---')
    resume_start = content.find('---RESUME---')
    difficulty_start = content.find('Difficulty Level:')

    jd_text = content[jd_start:resume_start].strip() if jd_start != -1 and resume_start != -1 else ""
    resume_text = content[resume_start:difficulty_start].strip() if resume_start != -1 and difficulty_start != -1 else ""
    difficulty = content[difficulty_start:].split("\n")[0].replace("Difficulty Level:", "").strip() if difficulty_start != -1 else "easy"

    qa_list = extract_qa_pairs(content)
    return jd_text, resume_text, difficulty, qa_list

multitask_data = []

for filename in os.listdir(root_path):
    if filename.endswith(".txt"):
        filepath = os.path.join(root_path, filename)
        jd, resume, level, qa_list = parse_txt_file(filepath)

        for q, a, source, qtype in qa_list:
            q_prompt = f"[TASK:QUESTION] JD: {jd} ||| Resume: {resume} ||| Level: {level}"
            a_prompt = f"[TASK:ANSWER] JD: {jd} ||| Resume: {resume} ||| Question: {q.strip()}"

            multitask_data.append((q_prompt, q.strip()))
            multitask_data.append((a_prompt, a.strip()))

df = pd.DataFrame(multitask_data, columns=["input_text", "target_text"])

for i in range(10):
    print(f"INPUT:\n{df.iloc[i]['input_text']}\nTARGET:\n{df.iloc[i]['target_text']}\n{'-'*80}")

# Limit to first 50 documents (each doc gives 2 rows: one question, one answer)
df = df.iloc[:1000]  # 50 docs Ã— 2 rows each

dataset = Dataset.from_pandas(df)

from datasets import Dataset
from transformers import T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained("t5-base")
# dataset = Dataset.from_pandas(df)

def tokenize_function(example):
    input_enc = tokenizer(example["input_text"], padding="max_length", truncation=True, max_length=512)
    target_enc = tokenizer(example["target_text"], padding="max_length", truncation=True, max_length=128)
    input_enc["labels"] = [(l if l != tokenizer.pad_token_id else -100) for l in target_enc["input_ids"]]
    return input_enc

tokenized_dataset = dataset.map(tokenize_function)



from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments

model = T5ForConditionalGeneration.from_pretrained("t5-base")

output_dir = "/content/drive/MyDrive/t5_model_output"

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=50,
    per_device_train_batch_size=2,
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="no",
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

trainer.train()

model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

import re

def extract_from_single_txt(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()

    jd_start = content.find('--- JOB DESCRIPTION ---')
    resume_start = content.find('---RESUME---')
    difficulty_start = content.find('Difficulty Level:')

    jd = content[jd_start:resume_start].strip() if jd_start != -1 and resume_start != -1 else ""
    resume = content[resume_start:difficulty_start].strip() if resume_start != -1 and difficulty_start != -1 else ""
    difficulty = content[difficulty_start:].split("\n")[0].replace("Difficulty Level:", "").strip() if difficulty_start != -1 else "easy"

    pattern = re.compile(r'Direct Question:\s*(.*?)\s*Direct Answer:\s*(.*?)\s*Source:\s*(.*?)\s*Type:\s*(\d+)', re.DOTALL)
    qa_list = pattern.findall(content)

    return jd, resume, difficulty, qa_list

def generate_question_and_answer(jd, resume, difficulty):
    # Generate question
    prompt_q = f"[TASK:QUESTION] JD: {jd} ||| Resume: {resume} ||| Level: {difficulty}"


    input_ids_q = tokenizer(prompt_q, return_tensors="pt").input_ids.to(model.device)
    output_q = model.generate(input_ids=input_ids_q, max_length=64, num_beams=4)
    question = tokenizer.decode(output_q[0], skip_special_tokens=True)

    # Generate answer using the generated question
    prompt_a = f"[TASK:ANSWER] JD: {jd} ||| Resume: {resume} ||| Question: {question}"

    input_ids_a = tokenizer(prompt_a, return_tensors="pt").input_ids.to(model.device)
    output_a = model.generate(input_ids=input_ids_a, max_length=128, num_beams=4)
    answer = tokenizer.decode(output_a[0], skip_special_tokens=True)

    return question, answer

import pickle

# Save the model to a file
with open('c', 'wb') as f:
    pickle.dump(model, f)

import os

# Provide the full path to your model file
model_path = "model.pkl"

# Check if the file exists
if os.path.exists(model_path):
    print("Model file found!")
else:
    print("Model file not found.")

file_path = "/content/drive/MyDrive/final/data_cloud_jd0_resume0_easy.txt"
jd, resume, level, original_qa = extract_from_single_txt(file_path)

generated_q, generated_a = generate_question_and_answer(jd, resume, level)

print("Generated Question:")
print(generated_q)
print("\n Generated Answer:")
print(generated_a)


print("\n Original QA Sample for Comparison:")
for q, a, _, _ in original_qa[:1]:
    print("Original Q:", q.strip())
    print("Original A:", a.strip())

output_dir = "/content/drive/MyDrive/t5_model_output"

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load the tokenizer
tokenizer = T5Tokenizer.from_pretrained(output_dir)

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

print(" Model and tokenizer loaded successfully.")

